@article{https://doi.org/10.1002/brb3.3205,
author = {Sun, Rui and Cheng, Andy S. K. and Chan, Cynthia and Hsiao, Janet and Privitera, Adam J. and Gao, Junling and Fong, Ching-hang and Ding, Ruoxi and Tang, Akaysha C.},
title = {Tracking gaze position from EEG: Exploring the possibility of an EEG-based virtual eye-tracker},
journal = {Brain and Behavior},
volume = {n/a},
number = {n/a},
pages = {e3205},
keywords = {BSS, eye movement, high-density EEG, ICA, saccade, SOBI, smooth pursuit},
doi = {https://doi.org/10.1002/brb3.3205},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/brb3.3205},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/brb3.3205},
abstract = {Abstract Introduction Ocular artifact has long been viewed as an impediment to the interpretation of electroencephalogram (EEG) signals in basic and applied research. Today, the use of blind source separation (BSS) methods, including independent component analysis (ICA) and second-order blind identification (SOBI), is considered an essential step in improving the quality of neural signals. Recently, we introduced a method consisting of SOBI and a discriminant and similarity (DANS)-based identification method, capable of identifying and extracting eye movement–related components. These recovered components can be localized within ocular structures with a high goodness of fit (>95\%). This raised the possibility that such EEG-derived SOBI components may be used to build predictive models for tracking gaze position. Methods As proof of this new concept, we designed an EEG-based virtual eye-tracker (EEG-VET) for tracking eye movement from EEG alone. The EEG-VET is composed of a SOBI algorithm for separating EEG signals into different components, a DANS algorithm for automatically identifying ocular components, and a linear model to transfer ocular components into gaze positions. Results The prototype of EEG-VET achieved an accuracy of 0.920° and precision of 1.510° of a visual angle in the best participant, whereas an average accuracy of 1.008° ± 0.357° and a precision of 2.348° ± 0.580° of a visual angle across all participants (N = 18). Conclusion This work offers a novel approach that readily co-registers eye movement and neural signals from a single-EEG recording, thus increasing the ease of studying neural mechanisms underlying natural cognition in the context of free eye movement.}
}

